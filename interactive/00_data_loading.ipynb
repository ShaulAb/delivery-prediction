{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from extra_structs import categories_translations, special_characters\n",
    "from helpers import load_data_as_df, check_missing_values, time_based_train_test_split\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customers\n",
    "\n",
    "Let's start with the basic stats about the customers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = load_data_as_df('customers')\n",
    "assert customers_df.customer_id.nunique() == len(customers_df)\n",
    "assert customers_df.customer_unique_id.nunique() <= len(customers_df)\n",
    "customers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if customer_city column contains any special characters that should not be expected in a city name\n",
    "customers_df[~customers_df.customer_city.str.contains(r'^[a-zA-Z\\s\\'-]+$')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_values(customers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_customers = customers_df.customer_id.nunique()\n",
    "total_uq_customers = customers_df.customer_unique_id.nunique()\n",
    "\n",
    "print(f'Total customers: {total_customers}')\n",
    "print(f'Total unique customers: {total_uq_customers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many customers are there in each state?\n",
    "customer_states = pd.DataFrame(customers_df.customer_state.value_counts(normalize=True))\n",
    "\n",
    "# add to customer_states a cumsum column so that we can see how many states are required for good coverage\n",
    "customer_states['cumsum'] = customer_states.cumsum()\n",
    "customer_states.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many customers are there in each state?\n",
    "customer_cities = pd.DataFrame(customers_df.customer_city.value_counts(normalize=True))\n",
    "# add to customer_states a cumsum column so that we can see how many states are required for good coverage\n",
    "customer_cities['cumsum'] = customer_cities.cumsum()\n",
    "customer_cities.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geolocation\n",
    "\n",
    "We could merge the dataframes and visualize the customers on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocation_df = load_data_as_df('geolocation')\n",
    "geolocation_df.sort_values(\n",
    "    by=['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng'],\n",
    "    ascending=False).head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_values(geolocation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocation_df.geolocation_city.value_counts().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minutes\n",
    "\n",
    "Before merging with the customer data, there are at least 2 questions to address:\n",
    "1. The data is very large, and each postal code is translated to different lat, lon locations.\n",
    "2. City names are not standardized, e.g.:\n",
    "    - \"SÃ£o Paulo\" vs \"Sao Paulo\" \n",
    "    - \"sao joao do pau d%26apos%3balho\" vs \"sao joao do pau dbalho\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to replace special characters\n",
    "def replace_special_characters(city_name):\n",
    "    for special_char, replacement in special_characters.items():\n",
    "        city_name = city_name.replace(special_char, replacement)\n",
    "    return city_name\n",
    "\n",
    "geolocation_df['geolocation_city'] = geolocation_df['geolocation_city'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "geolocation_df['geolocation_city'] = geolocation_df['geolocation_city'].apply(replace_special_characters)\n",
    "\n",
    "geolocation_clean_df = geolocation_df.groupby(['geolocation_zip_code_prefix', 'geolocation_city']).agg(\n",
    "    geolocation_lat=('geolocation_lat', 'mean'),\n",
    "    geolocation_lng=('geolocation_lng', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "del geolocation_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocation_clean_df[geolocation_clean_df['geolocation_zip_code_prefix'] == 17970]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finish by removing duplicates for each `customer_zip_code_prefix` .\n",
    "As can be seen above, it is due to the fact that the city name is not standardized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_len = len(geolocation_clean_df)\n",
    "geolocation_clean_df = geolocation_clean_df.drop_duplicates(subset=['geolocation_zip_code_prefix'])\n",
    "diff = original_len - len(geolocation_clean_df)\n",
    "percent = round(diff / original_len * 100, 2)\n",
    "print(f'Removed {diff} ({percent}%) duplicates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31% is a pretty high percentage, due to lack of time I will not be able to address this.  \n",
    "This issue should be looked if there's time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Visualization\n",
    "\n",
    "We will now visualize the different locations on a map.  \n",
    "Since there are 15K data points, we will use a marker cluster and only a random sample of 10%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = geolocation_clean_df.sample(frac=0.1)\n",
    "\n",
    "# Create a base map\n",
    "map = folium.Map(location=[-23.5505, -46.6333], zoom_start=12)\n",
    "\n",
    "# Add a marker cluster to the map\n",
    "marker_cluster = MarkerCluster().add_to(map)\n",
    "errors = []\n",
    "error_count = 0\n",
    "# Add markers to the map based on the random_sample\n",
    "for index, row in random_sample.iterrows():\n",
    "    try:\n",
    "        folium.Marker(\n",
    "            location=[row['geolocation_lat'], row['geolocation_lng']],\n",
    "        ).add_to(marker_cluster)\n",
    "    except:\n",
    "        errors.append(index)\n",
    "        error_count += 1\n",
    "\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many errors did we get?\n",
    "error_pct = round(error_count / len(random_sample) * 100, 4)\n",
    "print(f'Error rate: {error_pct}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order Items\n",
    "\n",
    "We now inspect the items data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_df = load_data_as_df('order_items')\n",
    "order_items_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uq_products = order_items_df.product_id.nunique()\n",
    "uq_sellers = order_items_df.seller_id.nunique()\n",
    "\n",
    "print(f'Unique products: {uq_products}')\n",
    "print(f'Unique sellers: {uq_sellers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many products per seller\n",
    "order_items_df.groupby('seller_id').product_id.nunique().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(order_items_df, x=\"freight_value\", marginal=\"box\") \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(order_items_df, x=\"price\", marginal=\"box\") \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_df['freight_component'] = order_items_df.freight_value / order_items_df.price\n",
    "order_items_df.freight_component.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be interesting to see if for some products the *freight price* is actually higher than the product price itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = order_items_df.freight_component > 1\n",
    "order_items_df[msk].freight_component.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Freight Component**\n",
    "\n",
    "We see that indeed in some cases the freight price is greater than the price of the item. \n",
    "As a sanity check, we should sample and check some of these items with SME."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orders Dataset\n",
    "\n",
    "Now we inspect the `Orders` dataset.\n",
    "\n",
    "This dataset as a lot of temporal features, and it should show us the issue that the product manager talked about - delivery delays.\n",
    "\n",
    "While we will perform this analysis in the next notebook, we will currently just add the time delta features and get an overview of the order cycle times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = load_data_as_df('orders')\n",
    "orders_df['order_approved_at'] = pd.to_datetime(orders_df['order_approved_at'])\n",
    "orders_df['order_delivered_carrier_date'] = pd.to_datetime(orders_df['order_delivered_carrier_date'])\n",
    "orders_df['order_delivered_customer_date'] = pd.to_datetime(orders_df['order_delivered_customer_date'])\n",
    "orders_df['order_estimated_delivery_date'] = pd.to_datetime(orders_df['order_estimated_delivery_date'])\n",
    "orders_df['order_purchase_timestamp'] = pd.to_datetime(orders_df['order_purchase_timestamp'])\n",
    "orders_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_values(orders_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing value in the dates data makes sense if the order lifecycle was not completed. \n",
    "However, just to make sure the NAs correspond to the correct logic, we will double-check the order status for these orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df[orders_df.isna().any(axis=1)].order_status.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df[orders_df.isna().any(axis=1) & (orders_df.order_status == 'shipped')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like some of the orders were shipped a very long time ago.\n",
    "These orders may have been delievered, but the data was not updated.\n",
    "Or they were never delievered, in this case it would be interesting to know why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.order_status.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Order Status**\n",
    "\n",
    "Overall we see pretty good stats here, the cancellation rate is extremely low for this kind of platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the orders time range, based on order_purchase_timestamp\n",
    "first_order = orders_df.order_purchase_timestamp.min()\n",
    "last_order = orders_df.order_purchase_timestamp.max()\n",
    "print(f\"Orders time range: {first_order} - {last_order}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot orders over time\n",
    "\n",
    "# Group by month and count the number of orders\n",
    "orders_per_month = orders_df.resample('ME', on='order_purchase_timestamp').size().reset_index(name='orders_count')\n",
    "\n",
    "# Create a line plot\n",
    "\n",
    "\n",
    "fig = px.line(orders_per_month, x='order_purchase_timestamp', y='orders_count', \n",
    "              title='Number of Orders per Month', \n",
    "              labels={'order_purchase_timestamp': 'Month', 'orders_count': 'Number of Orders'})\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Growth\n",
    "\n",
    "The number of orders per month has grown pretty well during the first year since launch.  \n",
    "Starting 2018, the number of orders reached a plateau and has been pretty stable since then.\n",
    "Finally, it looks like Sep 2018 data was not updated just yet.\n",
    "\n",
    "It would be interesting to talk with the product manager to understand the product strategy and see if it aligns with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert orders_df.order_id.nunique() == len(orders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding time delta features: \n",
    "#  - Time between order_purchase_timestamp and order_approved_at (approval time)\n",
    "#  - Time between order_approved_at and order_delivered_carrier_date delivery\n",
    "#  - Time between order_delivered_carrier_date and order_delivered_customer_date\n",
    "#  - Time between order_approved_at and order_delivered_customer_date (our target variable)\n",
    "# Results are in days\n",
    "\n",
    "seconds_in_day = 60 * 60 * 24\n",
    "def add_time_delta_features(df: pd.DataFrame, denominator: int = seconds_in_day):\n",
    "    df['approval_time'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / denominator\n",
    "    df['approved_to_carrier'] = (df['order_delivered_carrier_date'] - df['order_approved_at']).dt.total_seconds() / denominator\n",
    "    df['carrier_to_customer'] = (df['order_delivered_customer_date'] - df['order_delivered_carrier_date']).dt.total_seconds() / denominator\n",
    "    df['approval_to_customer'] = (df['order_delivered_customer_date'] - df['order_approved_at']).dt.total_seconds() / denominator\n",
    "    return df\n",
    "\n",
    "orders_df = add_time_delta_features(orders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation checks:\n",
    "#   all the time deltas should be non-negative\n",
    "def get_negative_time_deltas(df: pd.DataFrame):\n",
    "    data_anomalies = pd.DataFrame()\n",
    "    for col in ['approval_time', 'approved_to_carrier', 'carrier_to_customer', 'approval_to_customer']:\n",
    "        mask = df[col] < 0\n",
    "        if mask.any():\n",
    "            print(f\"{col} has {mask.sum()} negative values\")\n",
    "            data_anomalies = pd.concat([data_anomalies, df[mask]])\n",
    "\n",
    "    return data_anomalies\n",
    "\n",
    "delta_anomalies = get_negative_time_deltas(orders_df)\n",
    "if len(delta_anomalies) > 0:\n",
    "    delta_anomalies.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_time_anomlies = round(len(delta_anomalies) / len(orders_df) * 100, 2)\n",
    "print(f\"Percentage of time delta anomalies: {delta_time_anomlies}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some dates could be wrong, because they don't make sense with the order lifecycle.  \n",
    "Anyway, it is less than 1.5% of the data, which is not negligible, but seems manageable.\n",
    "\n",
    "We need to look into these cases more closely in the future and see if we can find the root cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the anomalies from the original `orders` dataframe\n",
    "orders = orders_df[~orders_df.index.isin(delta_anomalies.index)]\n",
    "del delta_anomalies\n",
    "gc.collect()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a boxplot for each of the time delta features\n",
    "fig = px.box(orders, y=['approval_time',\n",
    "                        'approved_to_carrier',\n",
    "                        'carrier_to_customer',\n",
    "                        'approval_to_customer'],\n",
    "                        title='Order Lifecycle Time Deltas',\n",
    "                        labels={'value': 'Time In Days'})\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(orders, x='approval_to_customer',\n",
    "             title='Purchased to Delieverd Time Deltas',\n",
    "             labels={'value': 'Time In Days'})\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Products Dataset\n",
    "\n",
    "Now we take a look at the contents that are traded in the marketplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = load_data_as_df('products')\n",
    "products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert products_df.product_id.nunique() == len(products_df), \"duplicated product IDs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_values(products_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df.product_category_name.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the product_category_name with the English translation\n",
    "products_df = products_df.merge(categories_translations, how='left')\n",
    "products_df.drop(columns=['product_category_name'], inplace=True)\n",
    "products_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncategorized = len(products_df[pd.isna(products_df.product_category)])\n",
    "uncategoried_ratio = round(uncategorized / len(products_df) * 100, 2)\n",
    "print(f\"Percentage of uncategorized products: {uncategoried_ratio}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, having uncategorized products should be discouraged.  \n",
    "We can try to correcly categorize the uncategorized products, but since the percentage is low, we will ignore this for now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df['total_size'] = products_df.product_length_cm * products_df.product_height_cm * products_df.product_width_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert order_items_df.product_id.nunique() == products_df.product_id.nunique(), \"Mismatch in product_id between order_items and products_df\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the total size of the products\n",
    "fig = px.histogram(products_df, x='total_size', title='Total Size Histogram')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df.product_category.value_counts(normalize=True).head(10).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize product weight\n",
    "fig = px.histogram(products_df, x='product_weight_g', title='Product Weight Histogram')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sellers Dataset\n",
    "\n",
    "Finally, we take a look at the sellers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers_df = load_data_as_df('sellers')\n",
    "sellers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sellers_df.seller_id.nunique() == len(sellers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_values(sellers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers_df.seller_city.value_counts(normalize=True).cumsum().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers_df.seller_city.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the DataFrames\n",
    "\n",
    "Now we merge all the dataframes to provide the data for EDA and Modelling.\n",
    "\n",
    "The merge will be as follows:\n",
    "\n",
    " - `Sellers`, `Geo_location` by `zip_code_prefix`\n",
    " - `Customers`, `Geo_location` by `zip_code_prefix`\n",
    " - `Order_items`, `products` by `product_id`\n",
    " - `Orders`, `order_items` by `order_id`\n",
    " - `Orders`, `sellers` by `seller_id`\n",
    " - `Orders`, `customers` by `customer_id`\n",
    "\n",
    "Everntaually, our `Orders` dataframe will be the main dataframe that will contain all the information. \n",
    "Do note, that even though we will perform *left* joins to `Orders`, the number of rows  **will increase** since a single order could contain multiple items and sellers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers_extended_df = sellers_df.merge(geolocation_clean_df, how='left', \n",
    "                                       left_on='seller_zip_code_prefix',\n",
    "                                       right_on='geolocation_zip_code_prefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename geolocation_ prefix to _customer prefix in all relevant columns\n",
    "sellers_extended_df = sellers_extended_df.rename(columns={\n",
    "    'geolocation_lat': 'seller_lat',\n",
    "    'geolocation_lng': 'seller_lng',\n",
    "})\n",
    "\n",
    "sellers_extended_df.drop(columns=[\"geolocation_zip_code_prefix\", \"geolocation_city\"], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_extended_df = customers_df.merge(geolocation_clean_df, how='left', \n",
    "                                       left_on='customer_zip_code_prefix',\n",
    "                                       right_on='geolocation_zip_code_prefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename geolocation_ prefix to _customer prefix in all relevant columns\n",
    "customers_extended_df = customers_extended_df.rename(columns={\n",
    "    'geolocation_lat': 'customer_lat',\n",
    "    'geolocation_lng': 'customer_lng',\n",
    "})\n",
    "\n",
    "customers_extended_df.drop(columns=[\"geolocation_zip_code_prefix\", \"geolocation_city\"], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_extended_df = order_items_df.merge(products_df, how='left', on='product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sellers_df\n",
    "del customers_df\n",
    "del geolocation_clean_df\n",
    "del order_items_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df_extended = orders_df.merge(order_items_extended_df, how='left', on='order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_extended_df = orders_df_extended.merge(sellers_extended_df, how='left', on='seller_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_extended_df = orders_extended_df.merge(customers_extended_df, how='left', on='customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundant columns like the seller_zip_code_prefix, ustomer_zip_code_prefix and geolocation_city\n",
    "orders_extended_df.drop(columns=[col for col in orders_extended_df.columns if col.endswith('_prefix')], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_extended_df.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide the data into training and test sets\n",
    "\n",
    "Now, we want to dive deeper into the EDA.  \n",
    "In order to make the test set close to the real world scenario, we will use the latest data as the test set. \n",
    "\n",
    "Also, since there are only 2 years of data, yearly seasonality will not be considered for modelling (but weekly / monthly seasonality will be considered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data - most recent 20% as test set\n",
    "train_df, test_df = time_based_train_test_split(orders_extended_df, 'order_purchase_timestamp', test_size=0.2)\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples ({len(train_df)/len(orders_extended_df):.1%})\")\n",
    "print(f\"Test set: {len(test_df)} samples ({len(test_df)/len(orders_extended_df):.1%})\")\n",
    "\n",
    "# Verify the split by checking date ranges\n",
    "print(f\"Train date range: {train_df['order_purchase_timestamp'].min()} to {train_df['order_purchase_timestamp'].max()}\")\n",
    "print(f\"Test date range: {test_df['order_purchase_timestamp'].min()} to {test_df['order_purchase_timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../data/train_df.csv', index=False)\n",
    "test_df.to_csv('../data/test_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
